{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kannada MNIST Knowledge Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Kaggle Knowledge competition is  to practice training convolutional neural networks (CNN) using dataset other than famous MNIST dataset. The dataset used in the competition is the recently-released dataset of Kannada digits. Kannada is a language spoken predominantly by people of Karnataka in southwestern India. The language has roughly 45 million native speakers and is written using the Kannada script. Extensive information about the language and its speakers can be found at\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kannada\n",
    "\n",
    "Tha dataset consists of 10 distinct digits that have no resemblance to the usual arabic numerals.\n",
    "One can see how the Kannada digits look like as well as download the original datasets by visiting the Kaggle webpage\n",
    "\n",
    "https://www.kaggle.com/c/Kannada-MNIST\n",
    "\n",
    "\n",
    "The code provided below was used in the aforementioned competition. This competition was Knowlege-type one, and its purpose is to practice rather than to find hyperparameters leading to a perfect score. The competition was kernel-based meaning that one had to commit and run the whole code first on the Kaggle provided GPU before generating a submission file. The submission was scored on both the public test set, as well as a private (unseen) test set. Since it takes enormous amount of time \n",
    "to run it on CPU, it is feasible to run it only if one has access to GPU.\n",
    "\n",
    "The CNN architecture provided here allowed to achieve the accuracies 0.99851 and 0.99430 on the custom made training and evaluation sets, and 0.98820 on both public and private test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and preparing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,Dropout,BatchNormalization\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read and examine the data. There are two datasets (that consist of the the pixel-values of the associated images) train.cv and Dig-MNIST.cv. Though one can use one set for training and the other for validation, we combine them in a single dataset, that will be split after shuffling into self-made training, validation and evaluation sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = pd.read_csv('.>/input/Kannada-MNIST/train.csv')\n",
    "#eval_df = pd.read_csv('../input/Kannada-MNIST/Dig-MNIST.csv')\n",
    "#test_df = pd.read_csv('../input/Kannada-MNIST/test.csv')\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "eval_df = pd.read_csv('./data/Dig-MNIST.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, eval_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seee that we have the completely balanced datasets in terms of the numbers of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_df['label'].value_counts().sort_index().to_dict())\n",
    "#print(eval_df['label'].value_counts().sort_index().to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for putting them into the deep network. The images are 28 times 28 in size being gray-scale and thus having only one channel. We first prepare the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28\n",
    "N_CHANNELS = 1 \n",
    "\n",
    "X = train_df.iloc[:, 1:].values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS).\\\n",
    "                astype('float32')/255\n",
    "\n",
    "X_test = test_df.iloc[:, 1:].values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS).\\\n",
    "                astype('float32')/255\n",
    "\n",
    "# print(X[145,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we convert the labels using one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(train_df.iloc[:, 0].values)\n",
    "\n",
    "#print(y[556])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then shuffle the combined dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(X, y, random_state = 736)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After shuffling, we leave 75 percent of the data for training and 25 percent for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size = 0.25, stratify = y, \n",
    "                                                  random_state = 6743)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define some constants that will be used in the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SHAPE = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "\n",
    "VERBOSE = 1\n",
    "\n",
    "#BATCH_SIZE = 256\n",
    "BATCH_SIZE = 512\n",
    "#BATCH_SIZE = 1024\n",
    "\n",
    "EPOCHS = 40\n",
    "#EPOCHS = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the keras provided ImageDataGenerator to augment the dataset in real time. Using the generator allows us to notably increase the validation and test accuracies. The optimal parameters such as rotation range in degrees as well as others can be seen from below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rotation_range = 11,\n",
    "                                   width_shift_range = 0.25,\n",
    "                                   height_shift_range = 0.25,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.3,\n",
    "                                   horizontal_flip = False,\n",
    "                                   vertical_flip = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = ImageDataGenerator() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will construct the CNN model to train and evaluate the data. The architecture of CNN is as follows:\n",
    "\n",
    "(i) Three convolutional layers each having 64 feature maps and followed by batch normalization.\n",
    "\n",
    "(ii) 2D pooling layer with 2x2 filter and 0.25 dropout.\n",
    "\n",
    "(iii) Three convolutional layers each having 128 feature maps and followed by batch normalization.\n",
    "\n",
    "(iv) 2D pooling layer with 2x2 filter and 0.25 dropout.\n",
    "\n",
    "(v) Two convolutional layers each having 256 feature maps and followed by batch normalization.\n",
    "\n",
    "(vi) 2D pooling layer with 2x2 filter and 0.25 dropout.\n",
    "\n",
    "(vii) After flattening, we add the Dense layer of size 512 and 0.5 dropout.\n",
    "\n",
    "(viii) Finally, the Dense layer with 10 nodes (number of classes) and softmax activation is used to make the final decision.\n",
    "\n",
    "RELu activations and He uniform initializers for weights were used everywhere except the last layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 7, 7, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,518,410\n",
      "Trainable params: 2,516,234\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     input_shape=INPUT_SHAPE, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                 padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     input_shape=INPUT_SHAPE, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     input_shape=INPUT_SHAPE, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                 padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     input_shape=INPUT_SHAPE, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     input_shape=INPUT_SHAPE, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "                 padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has 2,518,410 parameters. We then split the training dataset on the training set itself and validation set. The size of the validation part is 20 percent, meaning that the size of purely training set is 60 percent (25 persent were left for evaluation after training is complete earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VALIDATION_SPLIT = 0.25\n",
    "\n",
    "X_tr, X_valid, y_tr, y_valid = train_test_split(X_train, y_train, test_size = 0.2, \n",
    "                                        stratify = y_train, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step compile the model using categorical crossentropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_LR = 0.0025\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr = INITIAL_LR),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=Nadam(),\n",
    "#                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is  fitted using the checkpoint for each epoch. Best model based on validation accuracy will be used for prediction. The RMSppprop optimizer and variable learning rate are used in the process of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "def lr_decay(epoch):\n",
    "    return INITIAL_LR * 0.96 ** epoch\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.hdf5', monitor='val_accuracy', \n",
    "                            verbose=VERBOSE, save_best_only=True, mode='max')\n",
    "\n",
    "#checkpoint = ModelCheckpoint('/kaggle/working/weights.hdf5', monitor='val_accuracy', \n",
    "#                             verbose=VERBOSE, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks_list = [checkpoint, LearningRateScheduler(lr_decay)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "83/83 [==============================] - 2502s 30s/step - loss: 2.8719 - accuracy: 0.4686 - val_loss: 1.1529 - val_accuracy: 0.7009\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.70089, saving model to weights.hdf5\n",
      "Epoch 2/40\n",
      "83/83 [==============================] - 2467s 30s/step - loss: 0.4841 - accuracy: 0.8558 - val_loss: 25.2596 - val_accuracy: 0.1071\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.70089\n",
      "Epoch 3/40\n",
      "83/83 [==============================] - 2455s 30s/step - loss: 0.2368 - accuracy: 0.9311 - val_loss: 24.9768 - val_accuracy: 0.1250\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.70089\n",
      "Epoch 4/40\n",
      "83/83 [==============================] - 2470s 30s/step - loss: 0.1759 - accuracy: 0.9507 - val_loss: 2.8915 - val_accuracy: 0.5060\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.70089\n",
      "Epoch 5/40\n",
      "83/83 [==============================] - 2469s 30s/step - loss: 0.1422 - accuracy: 0.9601 - val_loss: 0.2665 - val_accuracy: 0.9122\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.70089 to 0.91220, saving model to weights.hdf5\n",
      "Epoch 6/40\n",
      "83/83 [==============================] - 2480s 30s/step - loss: 0.1278 - accuracy: 0.9638 - val_loss: 0.0266 - val_accuracy: 0.9866\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.91220 to 0.98661, saving model to weights.hdf5\n",
      "Epoch 7/40\n",
      "83/83 [==============================] - 2459s 30s/step - loss: 0.1214 - accuracy: 0.9677 - val_loss: 0.0436 - val_accuracy: 0.9881\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.98661 to 0.98810, saving model to weights.hdf5\n",
      "Epoch 8/40\n",
      "83/83 [==============================] - 2460s 30s/step - loss: 0.1013 - accuracy: 0.9720 - val_loss: 0.0639 - val_accuracy: 0.9821\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.98810\n",
      "Epoch 9/40\n",
      "83/83 [==============================] - 2456s 30s/step - loss: 0.1003 - accuracy: 0.9724 - val_loss: 1.7237 - val_accuracy: 0.7292\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.98810\n",
      "Epoch 10/40\n",
      "83/83 [==============================] - 2464s 30s/step - loss: 0.0922 - accuracy: 0.9739 - val_loss: 3.6584e-04 - val_accuracy: 0.9821\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.98810\n",
      "Epoch 11/40\n",
      "83/83 [==============================] - 2457s 30s/step - loss: 0.0878 - accuracy: 0.9753 - val_loss: 0.0164 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.98810 to 0.99256, saving model to weights.hdf5\n",
      "Epoch 12/40\n",
      "83/83 [==============================] - 2458s 30s/step - loss: 0.0846 - accuracy: 0.9768 - val_loss: 0.1698 - val_accuracy: 0.9330\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.99256\n",
      "Epoch 13/40\n",
      "83/83 [==============================] - 2462s 30s/step - loss: 0.0776 - accuracy: 0.9793 - val_loss: 1.7582 - val_accuracy: 0.7351\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.99256\n",
      "Epoch 14/40\n",
      "83/83 [==============================] - 2457s 30s/step - loss: 0.0759 - accuracy: 0.9797 - val_loss: 6.0200e-04 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.99256 to 0.99405, saving model to weights.hdf5\n",
      "Epoch 15/40\n",
      "83/83 [==============================] - 2458s 30s/step - loss: 0.0810 - accuracy: 0.9783 - val_loss: 0.0141 - val_accuracy: 0.9836\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.99405\n",
      "Epoch 16/40\n",
      "83/83 [==============================] - 2457s 30s/step - loss: 0.0708 - accuracy: 0.9791 - val_loss: 0.1518 - val_accuracy: 0.9367\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.99405\n",
      "Epoch 17/40\n",
      "83/83 [==============================] - 2467s 30s/step - loss: 0.0697 - accuracy: 0.9809 - val_loss: 0.0070 - val_accuracy: 0.9851\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.99405\n",
      "Epoch 18/40\n",
      "83/83 [==============================] - 2485s 30s/step - loss: 0.0625 - accuracy: 0.9825 - val_loss: 0.0129 - val_accuracy: 0.9881\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.99405\n",
      "Epoch 19/40\n",
      "83/83 [==============================] - 2477s 30s/step - loss: 0.0661 - accuracy: 0.9823 - val_loss: 0.0193 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.99405\n",
      "Epoch 20/40\n",
      "83/83 [==============================] - 2465s 30s/step - loss: 0.0611 - accuracy: 0.9829 - val_loss: 0.0364 - val_accuracy: 0.9881\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.99405\n",
      "Epoch 21/40\n",
      "83/83 [==============================] - 2448s 29s/step - loss: 0.0596 - accuracy: 0.9838 - val_loss: 1.5159e-04 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.99405 to 0.99554, saving model to weights.hdf5\n",
      "Epoch 22/40\n",
      "83/83 [==============================] - 2474s 30s/step - loss: 0.0571 - accuracy: 0.9840 - val_loss: 1.7918e-06 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.99554\n",
      "Epoch 23/40\n",
      "83/83 [==============================] - 2453s 30s/step - loss: 0.0552 - accuracy: 0.9846 - val_loss: 0.0022 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.99554\n",
      "Epoch 24/40\n",
      "83/83 [==============================] - 2454s 30s/step - loss: 0.0559 - accuracy: 0.9834 - val_loss: 0.9435 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.99554\n",
      "Epoch 25/40\n",
      "83/83 [==============================] - 2460s 30s/step - loss: 0.0537 - accuracy: 0.9853 - val_loss: 0.0028 - val_accuracy: 0.9970\n",
      "\n",
      "Epoch 00025: val_accuracy improved from 0.99554 to 0.99702, saving model to weights.hdf5\n",
      "Epoch 26/40\n",
      "83/83 [==============================] - 2469s 30s/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0121 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.99702\n",
      "Epoch 27/40\n",
      "83/83 [==============================] - 2466s 30s/step - loss: 0.0528 - accuracy: 0.9852 - val_loss: 4.9282e-06 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.99702\n",
      "Epoch 28/40\n",
      "83/83 [==============================] - 2467s 30s/step - loss: 0.0489 - accuracy: 0.9858 - val_loss: 0.1191 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.99702\n",
      "Epoch 29/40\n",
      "83/83 [==============================] - 2477s 30s/step - loss: 0.0490 - accuracy: 0.9862 - val_loss: 0.0224 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.99702\n",
      "Epoch 30/40\n",
      "83/83 [==============================] - 2478s 30s/step - loss: 0.0438 - accuracy: 0.9877 - val_loss: 0.0213 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.99702\n",
      "Epoch 31/40\n",
      "83/83 [==============================] - 2458s 30s/step - loss: 0.0473 - accuracy: 0.9865 - val_loss: 0.0012 - val_accuracy: 0.9940\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.99702\n",
      "Epoch 32/40\n",
      "83/83 [==============================] - 2466s 30s/step - loss: 0.0469 - accuracy: 0.9863 - val_loss: 4.4258e-04 - val_accuracy: 0.9985\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.99702 to 0.99846, saving model to weights.hdf5\n",
      "Epoch 33/40\n",
      "83/83 [==============================] - 2465s 30s/step - loss: 0.0470 - accuracy: 0.9871 - val_loss: 0.0040 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.99846\n",
      "Epoch 34/40\n",
      "83/83 [==============================] - 2468s 30s/step - loss: 0.0401 - accuracy: 0.9875 - val_loss: 0.0022 - val_accuracy: 0.9881\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.99846\n",
      "Epoch 35/40\n",
      "83/83 [==============================] - 2462s 30s/step - loss: 0.0418 - accuracy: 0.9877 - val_loss: 0.0020 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.99846\n",
      "Epoch 36/40\n",
      "83/83 [==============================] - 2460s 30s/step - loss: 0.0431 - accuracy: 0.9882 - val_loss: 4.1929e-05 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.99846\n",
      "Epoch 37/40\n",
      "83/83 [==============================] - 2459s 30s/step - loss: 0.0423 - accuracy: 0.9880 - val_loss: 6.4303e-04 - val_accuracy: 0.9985\n",
      "\n",
      "Epoch 00037: val_accuracy improved from 0.99846 to 0.99851, saving model to weights.hdf5\n",
      "Epoch 38/40\n",
      "83/83 [==============================] - 2463s 30s/step - loss: 0.0387 - accuracy: 0.9886 - val_loss: 0.1108 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.99851\n",
      "Epoch 39/40\n",
      "83/83 [==============================] - 2459s 30s/step - loss: 0.0399 - accuracy: 0.9891 - val_loss: 4.2860e-04 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.99851\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 2463s 30s/step - loss: 0.0367 - accuracy: 0.9892 - val_loss: 1.7668e-04 - val_accuracy: 0.9955\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.99851\n",
      "Wall time: 1d 3h 23min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_datagen.flow(X_tr, y_tr, batch_size = BATCH_SIZE),\n",
    "      steps_per_epoch = int(np.ceil(X_tr.shape[0]/BATCH_SIZE)),\n",
    "      epochs = EPOCHS,\n",
    "      callbacks = callbacks_list,\n",
    "      validation_data = valid_datagen.flow(X_valid, y_valid),\n",
    "      validation_steps =  int(np.ceil(X_valid.shape[0]/BATCH_SIZE)), \n",
    "      verbose = VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17560/17560 [==============================] - 318s 18ms/step\n",
      "Score on evalution set: 0.026426915126011568\n",
      "Accuracy on evaluation set: 0.9943052530288696\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_eval, y_eval,verbose = VERBOSE)\n",
    "\n",
    "print(\"Score on evalution set:\", score[0])\n",
    "print('Accuracy on evaluation set:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of training during 40 epochs, we achieved the accuracy of 0.994305 on the evaluation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "\n",
    "model.load_weights('weights.hdf5')\n",
    "#model.load_weights('/kaggle/working/weights_v5d.hdf5')\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# creating submission file\n",
    "subm = pd.DataFrame({'id': test_df.iloc[:,0].values,\n",
    "                       'label': y_pred_classes})\n",
    "\n",
    "subm.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model on the Kaggle provided test set (which is a public test set) gave the accuracy of 0.98820. The same accuracy was obtained when the model was applied to the private test set to form the private leaderboard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
